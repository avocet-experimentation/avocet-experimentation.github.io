---
title: Background
description: This section explains the challenges of software experimentation, compares different experimental design methods like A/B and switchback testing, and discusses existing solutions in the market.
---

Avocet is an open-source feature flagging and software experimentation platform capable of integrating with any telemetry infrastructure. As a feature flagging platform, it enables developers to distribute new features and changes to end users with the ability to easily and rapidly revert changes from a GUI, instead of having to change code, re-build, and then redeploy the application.

This allows developers to distribute new features with less of the normally associated risks such as unintended negative effects on user experience. Avocet offers support for switchback experimental designs in addition to the more common A/B test, making it an ideal solution for teams seeking to quickly begin experimenting on JavaScript applications where telemetry already exists and A/B tests are not viable.

To better understand Avocet’s use case, we'll look at why experimentation can be useful for software, what A/B and switchback designs are, and what situations call for one of these experimental designs over the other.

## Problem Overview

Commercial software can quickly grow too complex for developers to understand how a new feature or other change could impact user experience and profitability. Without this understanding, teams may take on significant risk when releasing such changes. To manage this risk, they can conduct empirical research by offering multiple versions of their application and statistically analyzing the results along key metrics such as user engagement. Normally, offering multiple versions of an app entails significant development work, even for a single feature, as they have to build, deploy, and maintain each version. Even more is required if any additional features are to be added, since this work has to be replicated to each version.

Feature flagging greatly simplifies this process, since a single build that utilizes it can offer many versions. Experimentation in turn becomes much easier since experimental treatments (also referred to as “interventions” or “independent variables”) can be defined as a set of flags and the values to set for them during a period of time for a given group of subjects, while observations (dependent variables) can be stored on telemetry data (metrics, logs, or traces).

Designing a sound experiment can be tricky, and each experiment needs its own design. Some design patterns are frequently useful, however, and have become staples in experimentation across disciplines. The most popular experimental design, the A/B test, involves subjects randomly assigned to one of two or more groups, with each group concurrently receiving a different treatment. This pattern is popular for a good reason, but there is a significant set of research questions that it does not serve well. In another design, the switchback test (aka reversal design), first the treatment being evaluated is applied to all subjects, and then the control is applied (in other words, subjects are switched back to the baseline). This sequence is then repeated at least once to improve the validity of results. While switchback tests are less common than A/B tests in formal research, they are ubiquitous in everyday experimentation. If you’ve ever toggled a light switch on and then off a couple of times to see if it works, you’ve done a switchback test!

![AB vs. Switchback](/ab-vs-switchback.png)

There are many more patterns, each offering a different set of tradeoffs, and a comprehensive comparison of them is beyond the scope of this text. However, we can illustrate the importance of design choice by examining the tradeoff between A/B and switchback tests for one specific concern in experiment design: inter-condition effects. “Condition,” in this text, means “combination of group and treatment,” and inter-condition effects are causal interdependencies between conditions.

A sound experiment design avoids inter-condition effects or at least ensures that they can be properly accounted for when analyzing experiment data. Some such effects are short-lived but significant; A/B tests are vulnerable to these, while switchback tests are not, provided the effects wear off between successive treatments. Other inter-condition effects are negligible in the short-term, but become more significant over time; these pose a greater risk to validity in switchback experiments, since the conditions being compared happen at different times and as such are differentially impacted by those effects.

To be more concrete, an experimental intervention that modifies the behavior of an app-wide ecosystem (such as implementing surge pricing to shape demand in a two-way marketplace) will almost assuredly cause significant short-term effects for all users in the ecosystem, making A/B tests a poor choice. On the other hand, if user learning impacts behavior (such as novelty effects on user enthusiasm) and this can’t be mitigated well, switchback tests may not be the right choice.

## Existing Solutions

When conducting research for this project,, we examined existing applications in this domain to better understand their approaches to experimentation. We focused on GrowthBook, Statsig, and LaunchDarkly;  while they all offer feature flagging capabilities, their takes on experimentation differ significantly in terms of infrastructure requirements, statistical capabilities, and implementation complexity. Each platform makes distinct trade-offs between ease of setup, analytical power, and feature completeness that shape their suitability for different organizational needs.

GrowthBook provides a straightforward open-source foundation that can be self-hosted, though its advanced features like multi-armed bandits, feature flag rules, and visual experiment management require MongoDB and Redis infrastructure, plus deploying GrowthBook's provided dashboard application as a separate service application. While this additional infrastructure setup adds complexity, teams can minimize the overhead by connecting GrowthBook to their existing MongoDB and Redis instances if already present in their stack.

Statsig emphasizes quick setup with managed cloud infrastructure but follows a usage-based pricing model that can become costly at scale. It offers sophisticated features like switchback testing and network effect analysis out of the box, though leveraging these advanced capabilities effectively requires a solid understanding of experimental design and statistical analysis, often necessitating input from data scientists or experienced analysts.

LaunchDarkly, though primarily known for feature management, takes a more generalist approach to experimentation, offering basic A/B testing functionality within its comprehensive feature management suite. While it provides targeting rules and user segmentation capabilities out of the box, its experimentation features are more limited compared to dedicated testing platforms, lacking advanced statistical methods or specialized experiment types.

Drawing from these observations of existing platforms, we identified an opportunity to develop Avocet, an experimentation application that balances analytical capabilities with operational simplicity. Our design incorporates the strengths of these existing solutions while addressing their limitations, particularly focusing on making advanced experimentation features accessible without requiring extensive infrastructure setup or specialized statistical expertise.