---
title: Technical Challenges
---

## Implementing Runtime Type Validation

We initially used Mongoose for our APIs to interact with the database, but quickly encountered its limitations, which prompted us to consider alternatives. One key issue was Mongoose’s typing constraints, particularly its document transform function, which relies on long-standing workarounds like “as” assertions. Furthermore, Mongoose only supports type validation within Mongo queries, whereas we require type validation in other parts of our application, such as the dashboard, where feature flags and experiments are created.

We evaluated a few options:
- `io-ts`, a popular package for runtime validation, requires schema to be written in its own syntax, which is highly functional and yields less readable schema. This was not a good fit since we wanted an object-oriented schema which we could directly reference when developing Avocet.
- Deepkit Runtime Types was appealing since it works directly from native TS types, but it didn’t work with tsx or vitest. It worked on ts-node, but this introduced new problems when running Management API Server, and we deemed it not worth the cost to solve those problems when we had a known working solution (Zod) to fall back on.
- Zod would require us to rewrite all our types from the ground up as Zod schema, but against the alternatives, and based on repeated confusions that arose around the structure of feature flags and experiments, it was clearly worth the time. As a bonus it permits more precise type definitions than TypeScript (TS) itself does, permitting schema for nonnegative integers, non-empty strings, and so on.

Zod let us infer TS types directly from schema, but these types lacked the readable in-editor hints that TS offers. Writing TS types in parallel would work but then mean we no longer had a single source of truth for types, and any mismatch between a schema or the corresponding type would risk silent bugs. We eventually found a solution that let us maintain a single source of truth, runtime validation and readable type hints: instead of types or interfaces, we created classes that implemented those Zod schema, and used those classes for type annotations.

## Managing Extended References

As discussed earlier, one of our core engineering decisions was to denormalize parts of experiments per the extended reference pattern to speed up flag value calculation. However, this decision was made after we had already implemented data mappers, so we sought a solution that did not involve an extensive redesign.

To ensure all changes to documents are replicated to their extended references, we added methods to create/update/delete any extended references embedded into other documents, and then combined these with the primary document mutation into a transaction. If a query failed for either the main document or its extended references, the entire operation would be reversed, keeping them in sync.

Before we could implement methods on `ExperimentRepository` to override the placeholder methods, however, we had to address another limitation. Our data mappers siloed off each collection from the rest; each mapper worked with documents of a specific type from a specific collection. To allow cross-repository communication, we created a `RepositoryManager` class that instantiates each repository class, passes itself into repository constructors, and stores the repository instances on itself. Repositories in turn store the manager on themselves. This permitted any repository to be accessed from within methods on any other repository.

## Limiting Information Leakage when Sending Flag Data

When responding to client requests for flag values, cattails doesn’t send the flag object, but simply the final calculated value, and some metadata. This metadata lets us identify the override rule that caused the specific value. If the rule was an experiment, we would need to be able to identify which experiment, group, and treatment was selected.

The most straightforward way of doing this was to include the IDs of each of those in metadata, but we didn't want to expose any sensitive information. Groups and treatments use V4 UUIDs, which don’t leak information. However, since we store experiments as a top-level Mongo document type, their IDs are Object IDs, which begin with a timestamp in the first 4 bytes indicating their time of creation.

We also had to consider our data access patterns for experiment data. The main pattern was to select an experiment and then fetch the corresponding spans. However, we also wanted to retain the option of identifying a matching experiment given a span.

We considered the following options:
- Non-reversible hashing: this would obscure IDs and be fairly simple, but prevent efficient experiment lookup from a span.
- Symmetric encryption. This would require storing an encryption key that could be accessed by both sides of cattails.
- Send plain IDs, but include randomly generated IDs as needed so that the metadata always has the same number of IDs in it. This would prevent users from looking at the response body and figuring out they were enrolled in an experiment.

We ultimately decided to send plain IDs, as the risks were minor. Later, we may eliminate the small information leak (the timestamp at the start of each Object ID) by creating flags and experiments with UUIDs and using those instead of the object ID that Mongo creates.

## Developing Data APIs

We developed the dashboard and the management API in tandem. The management API underwent frequent changes during development, and this would often break corresponding data-fetching code on the dashboard. Furthermore, these components needed to work with several types of data (feature flags, experiments, environments, and SDK connections; and potentially also client property definitions and dashboard users), each of which would require several routes in the REST model. We decided to use GraphQL to reduce dashboard development friction, speed up API development, and reduce the management API to a single route.

This presented some drawbacks. For one, there were costs associated with learning and implementing GraphQL, which was a new tool to all of us. These costs were acceptable, however, because we already needed to
<a href="/case-study/engineering-decisions#integrate-with-any-telemetry-pipeline">use GraphQL to make our project work with any telemetry database</a>.
More significantly, the GraphQL schema we wrote for the admin API are essentially duplicates of our type definitions, and as such any changes to types in our core package needed to be replicated in the GraphQL schema. However, we could in the future infer GraphQL schema directly from our classes using
<a href="https://typegraphql.com/" target="_blank" rel="noopener">TypeGraphQL</a>.

GraphQL also offered some secondary advantages.
<a href="https://github.com/graphql/graphiql/tree/main/packages/graphiql" target="_blank" rel="noopener">GraphiQL</a>
(note the “i”) is a convenient interface for prototyping requests that provides a catalog of available queries and mutations as well as autocompletion features. This allowed all team members to understand how to phrase data fetching requests, whereas before GraphQL we instead needed to look at the management API’s code, or at documentation that was not always up to date since our routes evolved quickly. GraphQL also offers runtime type validation, by checking requests against its schema. Finally, the ecosystem of tools that GraphQL provides could enable some easy-to-implement extensions in the future, such as embedding a GraphiQL widget into the admin dashboard to empower users to make custom queries to the telemetry server with schema-driven hints, enhancing usability.
