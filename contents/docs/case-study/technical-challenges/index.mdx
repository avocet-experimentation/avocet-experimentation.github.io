---
title: Technical Challenges during Development
description: This section provides an overview of Background.
---

## Implementing Runntime Type Validation

We initially used Mongoose on cattails, but quickly encountered its limitations, which prompted us to consider alternatives. One key issue was Mongoose’s typing constraints, particularly its document transform function, which relies on long-standing workarounds like “as” assertions. Furthermore, Mongoose only supports type validation within Mongo queries, whereas we require type validation in other parts of our application, such as the dashboard, where feature flags and experiments are created.

We evaluated a few options:
- `io-ts`, a popular package for runtime validation, requires schema to be written in its own syntax, which is highly functional and yields less readable schema. This was not a good fit since we wanted an object-oriented schema which we could directly reference when developing Avocet.
- Deepkit Runtime Types was appealing since it works directly from native TS types, but it didn’t work with tsx or vitest. It worked on ts-node, but this introduced new problems when running Management API Server, and we deemed it not worth the cost to solve those problems when we had a known working solution (Zod) to fall back on.
- Zod would require us to rewrite all our types from the ground up as Zod schema, but against the alternatives, and based on repeated confusions that arose around the structure of feature flags and experiments, it was clearly worth the time. As a bonus it permits more precise type definitions than TypeScript (TS) itself does, permitting schema for nonnegative integers, non-empty strings, and so on.

Zod let us infer TS types directly from schema, but these types lacked the readable in-editor hints that TS offers. Writing TS types in parallel would work but then mean we no longer had a single source of truth for types, and any mismatch between a schema or the corresponding type would risk silent bugs. We eventually found a solution that let us maintain a single source of truth, runtime validation and readable type hints: instead of types or interfaces, we created classes that implemented those Zod schema, and used those classes for type annotations.

[Diagram]

## Managing Extended References

As discussed earlier, one of our core engineering decisions was to denormalize parts of experiments per the extended reference pattern to speed up flag value calculation. However, this decision was made after we had already implemented data mappers, so we sought a solution that did not involve an extensive redesign.

To ensure all changes to documents are replicated to their extended references, we added methods to create/update/delete any extended references embedded into other documents, and then combined these with the primary document mutation into a transaction. If a query failed for either the main document or its extended references, the entire operation would be reversed, keeping them in sync.

Before we could implement methods on `ExperimentRepository` to override the placeholder methods, however, we had to address another limitation. Our data mappers siloed off each collection from the rest; each mapper worked with documents of a specific type from a specific collection. To allow cross-repository communication, we created a `RepositoryManager` class that instantiates each repository class, passes itself into repository constructors, and stores the repository instances on itself. Repositories in turn store the manager on themselves. This permitted any repository to be accessed from within methods on any other repository.

## Limiting Information Leakage when Sending Flag Data

When responding to client requests for flag values, cattails doesn’t send the flag object, but simply the final calculated value, and some metadata. This metadata lets us identify the override rule that caused the specific value. If the rule was an experiment, we would need to be able to identify which experiment, group, and treatment was selected.

The most straightforward way of doing this was to include the IDs of each of those in metadata, but we didn't want to expose any sensitive information. Groups and treatments use V4 UUIDs, which don’t leak information. However, since we store experiments as a top-level Mongo document type, their IDs are Object IDs, which begin with a timestamp in the first 4 bytes indicating their time of creation.

We also had to consider our data access patterns for experiment data. The main pattern was to select an experiment and then fetch the corresponding spans. However, we also wanted to retain the option of identifying a matching experiment given a span.

We considered the following options:
- Non-reversible hashing: this would obscure IDs and be fairly simple, but prevent efficient experiment lookup from a span.
- Symmetric encryption. This would require storing an encryption key that could be accessed by both sides of cattails.
- Send plain IDs, but include randomly generated IDs as needed so that the metadata always has the same number of IDs in it. This would prevent users from looking at the response body and figuring out they were enrolled in an experiment.

We ultimately decided to send plain IDs, as the risks were minor. Later, we may eliminate the small information leak (the timestamp at the start of each Object ID) by creating flags and experiments with UUIDs and using those instead of the object ID that Mongo creates.

## Developing Data APIs

We developed the dashboard and the admin server in tandem. The admin server’s API changed frequently, and these changes typically caused corresponding data-fetching code on the dashboard to break. Furthermore, these components needed to work with many types of data (feature flags, experiments, environments, client property definitions, SDK connections, and dashboard users), each of which would require several routes on cattails in the REST model. We decided to use GraphQL to reduce this development friction, speed up API development, and reduce the admin API to a single route.

This presented some drawbacks, though. For one, there were costs associated with learning and implementing GraphQL, which was a new tool to all of us. These costs were acceptable, however, because we already needed to use GraphQL to make our project work with any telemetry database. More significantly, the GraphQL schema we wrote for the admin API are essentially duplicates of our type definitions, and as such any changes to types in our core package needed to be replicated in the GraphQL schema. However, we could set up GraphQL schema inference directly from our classes with TypeGraphQL.

GraphQL offered some other advantages, such as a convenient interface (GraphiQL; note the “i”) for trying out queries, along with a catalog of available queries and autocompletion features. This allowed all team members to understand how to write GraphQL requests without having to look at cattails’ code or at documentation that was often out of date as our routes evolved quickly in tandem with our needs. GraphQL also offered another form of runtime type validation by checking requests against its schema. Additionally, the ecosystem of tools that GraphQL provides could enable some easy-to-implement extensions in the future, such as embedding a GraphiQL widget into the admin dashboard to empower users to make custom queries with schema-driven hints, enhancing usability.

## Known Limitations

If a client fails to fetch the value of a feature flag, it falls back on null. Depending on the phrasing of conditional logic involving flag values, this may cause undesirable behavior. We recommend either explicitly handling the null case, or consistently using falsy flag values to fall back onto some known safe functionality.

Flag values are only updated when clients request them. If autorefresh is off, a flag will not be updated unless the SDK is reinitialized, which is typically only when the application restarts.

Browser applications present some risks. For one, users can tamper with feature flag gates. This is a limitation of frontend apps, however, rather than one specific to our application. Another risk is that SDK API keys can be leaked, since the key will be visible in outbound request bodies. This key could be used to execute denial of service attacks against the flagging API; protections such as CORS and cookies can be circumvented by spoofing the Origin header or parsing responses to get cookies and then assigning them. If either of these is a concern, we recommend moving sensitive functionality and SDK usage to server-side code.

Flags cannot be defined as dependent on other flags, which can become an issue if flags are used long-term to gate features and other newer features are built that rely on gated features. Dependent flags are still possible, however, by checking all flags upstream in a flag’s dependency chain.
